from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

#å¯¼å…¥ç¯å¢ƒ
import os
from dotenv import load_dotenv
load_dotenv()
qwen_api_key = os.getenv('QWEN_API_KEY')
# åŠ è½½ llm æ¨¡å‹
llm = ChatOpenAI(
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=qwen_api_key,	# app_key
    model="qwen-plus",	# æ¨¡å‹åç§°
    temperature=0,
)

prompt_template = ChatPromptTemplate.from_template(
    """ä½ æ˜¯ä¸€ä¸ªå’Œäººç±»å¯¹è¯çš„æœºå™¨äººåŠ©æ‰‹.

    {chat_history}

    Human: {human_input}
    Chatbot:"""
    )
memory = ConversationTokenBufferMemory(
    llm=llm,  # ğŸ’¡ ä½ å¿…é¡»ä¼ å…¥ä¸€ä¸ªæ”¯æŒ token è®¡æ•°çš„ LLMï¼ˆå¦‚ ChatOpenAIï¼‰
    max_token_limit=1000,  # ğŸ’¡ ä¸Šä¸‹æ–‡æœ€å¤§ token é™åˆ¶
    memory_key="chat_history",  # PromptTemplate ä¸­å ä½ç¬¦åå­—
    return_messages=True  # è‹¥ä¸º Trueï¼Œè¿”å› Message ç±»å‹ï¼Œé€‚é… ChatPromptTemplate
)
# chain = prompt_template | llm | memory #ç›®å‰ç®¡é“ç¬¦è¿˜ä¸æ”¯æŒmemory
chain = LLMChain(
    llm=llm,
    prompt=prompt_template,
    memory=memory,
)

# è¿›è¡Œå¯¹è¯ï¼Œè¿ç»­è°ƒç”¨ï¼Œllmä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯,ä½†åªä¼šä¿ç•™è®¾ç½®çš„kè½®
response_1 = chain.invoke({"human_input":"è¯·ä½ ä»‹ç»ä¸€ä¸‹ä¸­å¤®è´¢ç»å¤§å­¦"})
print(response_1.get("text"))

response_2 = chain.invoke({"human_input":"ä¸­å¤®è´¢ç»å¤§å­¦çš„æ ¡è®­æ˜¯ä»€ä¹ˆ"})
print(response_2.get("text"))

response_3 = chain.invoke({"human_input":"æˆ‘ç¬¬ä¸€ä¸ªé—®é¢˜é—®äº†ä»€ä¹ˆ"})
print(response_3.get("text"))